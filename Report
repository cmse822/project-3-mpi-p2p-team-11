
Part 1 Code:

Code: blocking_ping_pong.c 

Compilation: mpicc blocking_ping_pong.c -o blocking_ping_pong

Same-node job script: same_node_blocking.sb 

Different-node job script: different_node_blocking.sb
____________________________________________________________________________________________________________________________________

Part 2 Code:

Code: found in the part2 directory.

Compilation: mpicxx -o nonblocking_ping_pong nonblocking_ping_pong.cpp

Same-node job script and Different-node job scripts are the same as part 1 but with running the nonblocking executable.

nonblocking_ping_pong_time.pdf shows the communication time for non-blocking ping-pong on message size 2^1 to 2^16 and 
it was run on the amd20 developement node using 2 processes. 

In the next part, we run non-blocking ping-pong on same node and on 2 different nodes with message size 2^1 to 2^32 and compare it to the 
blocking ping-pong. We don't use the developement nodes in the next part but we request cluster resources using the 2 job scripts 
(same_node_blocking.sb and different_node_blocking.sb). 

____________________________________________________________________________________________________________________________________

Parts 1 and 2 Analysis:

-> We run our code on nodes in the amd20 cluster.

-> The communication time corresponds to the round-trip time of the exchange (time for a ping-pong exchange).

-> The message size was varied from 2^1 to  2^21 bytes (we run 100 iterations for each message size and compute the average time).

-> The plot (average communication time of a single exchange in seconds vs message size in bytes) for blocking and non-blocking ping-pong 
exchange utilizing both 1 node (both processes running on a single node) and 2 nodes (each process running on a different node) is shown in
in parts_1_and_2_plot1.png figure_1.

-> In parts_1_and_2_plot1.png figure_1, we can that running each process on a different node dramatically increases the running time for an
exchange as compared to running the two processes on the same node. This is because the local-area-network would be used for 
communication as opposed to shared memory communication mechanisms when running the two processes on the same node. 

-> We can also observe that non-blocking communication is faster than blocking communication for both same-node utilization and
different-node utilization. In figure_1, the communication time for blocking and non-blocking graphs aren't clear since the discrepancy 
between in-node and different-node communication experiments is very large as discussed above. In figure_2, we exclude the results
for the different node experiments to  show that non-blocking communication time is faster than blocking communication time when 
run on the same node as well.

Bandwidth Analysis: (We didn't use GiB/s ; used GB/s base 10)
-> For each of the 4 graphs in parts_2_and_3_plot1.png figure_1, we calculate the best-line fit to compute the bandwidth and 
latency where the bandwidth is the inverse of the slope and the latency is the y-intercept. We use the x values of message sizes 
from 2^1 bytes to 2^21 bytes. The graphs are shown in parts_2_and_3_plot2.png.

-> Both the bandwidths for the blocking and non-blocking communication on different-nodes are lower than both the bandwidths 
for the blocking and non-blocking communication on the same-node. This aligns with our analysis above. 

-> The bandwidth for same-node blocking communication is lower than the bandwidth for non-blocking same-node communication which 
aligns with our analysis above.

-> The bandwidth for different-node blocking communication is lower than the bandwidth for non-blocking different-node communication 
which aligns with our analysis above.

-> The Steeper the slopes are for the best-line fits the higher the bandwidth is since bandwidth is the inverse of the slope.

Note: We compute a best line fit where (T(n) = a + Bn ; a is the latency and B is the inverse of the
bandwidth (communication time in s per byte) 

Latency Analysis:

-> Fitting best-lines on all the 4 curves returns negative y-intercepts as shown in parts_2_and_3_plot2.png. For this reason,
we only consider the first 8 message sizes to fit a best-line for the purpose of getting the latency/y-intercept. 
All 4 latencies returned were positive. Using 2^1 to 2^8 bytes as message size make sense when computing the latency since 
small message sizes are needed to compute network latency. The graphs are shown in parts_2_and_3_plot3.png. 

-> The latency for both blocking and non-blocking different node communication is higher than the latency for both blocking and
non-blocking same node communication. This can be observed in the graphs where small message sizes for same node communication take 
less time than small message sizes for different node communications. 

-> The latency for same node blocking communication is lower than the latency for non-blocking same node communication which can be observed
in our graphs.

-> The latency for different node blocking communication is lower than the latency for non-blocking different node communication which
can be observed in our graphs.


____________________________________________________________________________________________________________________________________

Parts 3 and 4 Code:

Details found in the part3 and part4 directories. 

____________________________________________________________________________________________________________________________________

Parts 3 and 4 Analysis:

-> We run our code on nodes in the amd20 cluster.

-> The message size was varied from 2^1 to  2^21 bytes (we run 100 iterations for each message size and compute the average time).

-> There doesn't appear to be significant difference in communication time between the blocking and the non-blocking case. We expected
the non-blocking isend and recv to achieve better performance but this wasn't the case. This is maybe due to sendrecv being very optimized
by mpi so that implementing our own non-blocking communication using isend and irecv doesn't offer performance improvements like what we
observed in parts 1 and 2. That being said, the sendrecv blocking communication seems to be slightly  quicker for all varied ranks and
message sizes which is the case for most processor counts. Moreover, we observe that the higher the  processor count, the higher is the 
communication time. There is a sharp increase in the rate of change (time in s)/bytes  at message size 2^15 for both blocking and 
non-blocking communication. The sharp increase in the rate of change is followed by a consistent steep slope. This means that the bandwidth
at message 2^15 decreases sharply and then becomes stable/constant afterwards. The communication time rate of increase is the highest at
processor count 128 since more than 1 node will be utilized now (max number of cores is 128 per a single node). 



-> We tried testing the isendrecv mpi function instead of isend/irecv functions but it is only supported in the openmpi 5.0 version which
implements the MPI 4.0 standard. The openmpi 5.0 version isn't installed on the hpcc so we installed it locally on our hpcc account. When 
running the code, it was running fine but it was returning a warning that the network communication isn't able to use the Infiniband 
interface so it probably downgraded to the Ethernet interface since it was still being able to communicate across nodes. We ran the sendrecv
code again but using openmpi 5.0 (Ethernet interface) and compared it with isendrecv which was also ran using openmpi 5.0 (Ethernet interface).
We also didn't notice any significant differences between communication times except for large processor counts such as 128 processors.
The communication time results for isendrecv were 2.08197, 2.20092, 1.42546, 1.67711, 2.22386, 1.98281, 1.7421, 0.93896, 1.108, 1.79003, and
2.70619 for message sizes 2^10 to 2^20. The communication time resutls for sendrecv however were 1.86971, 2.23217, 2.04483, 1.99266, 1.52353, 
2.05799, 1.93897, 1.98355, 2.67182, 2.47717, and 3.49694 for message sizes 2^10 to 2^20. 

